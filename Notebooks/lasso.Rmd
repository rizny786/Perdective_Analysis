---
title: "Lasso regression modelling"
author: "Rizny Mubarak"
output: 
  pdf_document: 
    toc: yes
    toc_depth: 4
---

### 1. Load Data

```{r}
x <- ts(read.csv("./walmart.csv"),frequency=4,start=c(2003,1))
plot(x[,1],ylab="Walmart sales")

```

### 2. Build a benchmark regression

```{r}
y.trn <- window(x[,1],end=c(2013,4))
y.tst <- window(x[,1],start=c(2014,1))
```

```{r}
n <- length(y.trn)
X <- array(NA,c(n,6))
# Loop to create lags
for (i in 1:6){
X[i:n,i] <- y.trn[1:(n-i+1)]
}
#Name the columns
colnames(X) <- c("y",paste0("lag",1:5))
X <- as.data.frame(X)
head(X)
```

```{r}
# The complete model
fit1 <- lm(y~.,data=X)
# The stepwise model
fit2 <- step(fit1,trace=0)
summary(fit2)
```

```{r}
# In-sample fit:
plot(X$y,type="l")
frc <- predict(fit2,X)
lines(frc,col="red")
```

```{r}
# Initialise an array to save the forecasts
frc1 <- array(NA,c(8,1))
for (i in 1:8){
# For the Xnew we use the last five observations as before
Xnew <- tail(y.trn,5)
# Add to that the forecasted values
Xnew <- c(Xnew,frc1)
# Take the relevant 5 values. The index i helps us to get the right ones
Xnew <- Xnew[i:(4+i)]
# If i = 1 then this becomes Xnew[1:5].
# If i = 2 then this becomes Xnew[2:6] - just as the example above.
# Reverse the order
Xnew <- Xnew[5:1]
# Make Xnew an array and name the inputs
Xnew <- array(Xnew, c(1,5)) # c(1,5) are the dimensions of the array
colnames(Xnew) <- paste0("lag",1:5) # I have already reversed the order
# Convert to data.frame
Xnew <- as.data.frame(Xnew)
# Forecast
frc1[i] <- predict(fit2,Xnew)
}
frc1
```

```{r}
frc1 <- ts(frc1,frequency=frequency(y.tst),start=start(y.tst))
ts.plot(y.trn,y.tst,frc1,col=c("black","black","red"))
```

### 3. Lasso regression

```{r}
library(glmnet)
```

```{r}
xx <- as.matrix(X[-(1:5),-1])
# For the target I retain only the first column
yy <- as.matrix(X[-(1:5),1])
```

```{r}
lasso <- cv.glmnet(x=xx,y=yy)
```

```{r}
coef(lasso)
```

```{r}
plot(lasso)
```

```{r}
frc2 <- array(NA,c(8,1))
for (i in 1:8){
# Create inputs - note for lasso we do not transform these into data.frame
Xnew <- c(tail(y.trn,5),frc2)
Xnew <- (Xnew[i:(4+i)])[5:1]
Xnew <- array(Xnew, c(1,5))
colnames(Xnew) <- paste0("lag",1:5)
# Forecast
frc2[i] <- predict(lasso,Xnew)
}
```

```{r}
# Transform to time series
frc2 <- ts(frc2,frequency=frequency(y.tst),start=start(y.tst))
# Plot together with fit2
ts.plot(y.trn,y.tst,frc1,frc2,col=c("black","black","red","blue"))
legend("bottomright",c("OLS","Lasso"),col=c("red","blue"),lty=1)
```

```{r}
ridge <- cv.glmnet(x=xx,y=yy,alpha=0)
coef(ridge)
```

```{r}
cc <- as.matrix(cbind(coef(lasso),coef(ridge)))
colnames(cc) <- c("lasso","ridge")
round(cc,3)
```

### 4. Exercises on lasso regression

#### 4.1 Ridge regression

-   Forecast

```{r}
frc3 <- array(NA,c(8,1))
for (i in 1:8){
# Create inputs - note for lasso we do not transform these into data.frame
Xnew <- c(tail(y.trn,5),frc2)
Xnew <- (Xnew[i:(4+i)])[5:1]
Xnew <- array(Xnew, c(1,5))
colnames(Xnew) <- paste0("lag",1:5)
# Forecast
frc3[i] <- predict(ridge,Xnew)
}

```

-   Plot Ridge with other forecasts

```{r}
frc3 <- ts(frc3,frequency=frequency(y.tst),start=start(y.tst))
# Plot together with fit2
ts.plot(y.trn,frc1,frc2,frc3,col=c("black","black","red","blue","magenta"))
legend("bottomright",c("OLS","Lasso","Ridge"),col=c("red","blue","magenta"),lty=1)
```

#### 4.2 Exponential smoothing

```{r}
library(forecast)
```

-   Fit the ETS Model

```{r}
fit4 <- ets(y.trn)
summary(fit4)
```

-   Forecast using ETS

```{r}
frc4 <- forecast(fit4, h=8)
frc4 <- frc4$mean
```

-   Plot ETS wit others

```{r}
# Plot together with fit2
ts.plot(y.trn,y.tst,frc1,frc2,frc3,frc4,col=c("black","black","red","blue","magenta","green"))
legend("bottomright",c("OLS","Lasso","Ridge","ETS"),col=c("red","blue","magenta","green"),lty=1)
```

#### 4.3 Calculate Error

```{r}
actual <- matrix(rep(y.tst,4),ncol=4)
actual
```

```{r}
error <- abs(actual - cbind(frc1,frc2,frc3,frc4))
colnames(error) <- c("OLS", "Lasson", "Ridge", "ETS")
MAE <- colMeans(error)
MAE
```

### 5. Answer

::: {style="color:green"}
Among the models tested, the OLS model with autoregressive components emerged as the most predictive, exhibiting the lowest MAE and superior visual performance. In contrast, the Lasso model showed promise in feature selection and predictive precision but was less efficient than OLS. The ETS benchmark, renowned for accurate predictions, ranked third in performance, serving as a reliable reference despite not surpassing OLS and Lasso. However, despite its regularization advantages, the Ridge model demonstrated the lowest projected accuracy and poorest overall performance. In conclusion, this scientific study underscores the practical and analytical advantages of utilizing the OLS model with autoregressive components for precise sales value predictions, showcasing its effectiveness in capturing trends and correlations within sales data.
:::
